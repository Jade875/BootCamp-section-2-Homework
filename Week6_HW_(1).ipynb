{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jade875/BootCamp-section-2-Homework/blob/main/Week6_HW_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
      "metadata": {
        "id": "c7e50351-15c7-4379-9369-cd41cd7ac272"
      },
      "source": [
        "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
        "\n",
        "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
        "\n",
        "**Name:** Jadesola Kassim\n",
        "\n",
        "**Email:** Jek8857@nyu.edu\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
      "metadata": {
        "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8"
      },
      "source": [
        "### Problem 1: Dataset Splitting\n",
        "\n",
        "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
        "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
        "\n",
        "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
        "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
        "\n",
        "*Describe your proposed split strategy and reasoning.* (Theory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e",
      "metadata": {
        "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e"
      },
      "outputs": [],
      "source": [
        "#todo\n",
        "#First: Grouping by person and not recording.\n",
        "#Then train 70 people with 15 validating and 15 testing.\n",
        "#Each person's five days of audio stay together in their group, to ensure that the model nevel sees the same speaker and handles new voices.\n",
        "#Second: keep kilian separate from the others.\n",
        "#Group Killian's days/sessions by percentages: 50% train, 25% validation and 25% testing.\n",
        "#Third: Train a base model on the 70 train speakers\n",
        "#Adjust that model on Killian's 50% training data\n",
        "#Last: Perform test to show if the model still workd on new speakes.\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
      "metadata": {
        "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8"
      },
      "source": [
        "### Problem 2: K-Nearest Neighbors\n",
        "\n",
        "1. **1-NN Classification:** Given dataset:\n",
        "\n",
        "   Positive: (1,2), (1,4), (5,4)\n",
        "\n",
        "   Negative: (3,1), (3,2)\n",
        "\n",
        "   Plot the 1-NN decision boundary and classify new points visually.\n",
        "\n",
        "2. **Feature Scaling:** Consider dataset:\n",
        "\n",
        "   Positive: (100,2), (100,4), (500,4)\n",
        "\n",
        "   Negative: (300,1), (300,2)\n",
        "\n",
        "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
        "\n",
        "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
        "\n",
        "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
      "metadata": {
        "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8"
      },
      "outputs": [],
      "source": [
        "#In 1-NN, each new point is classified based on which training point is nearest. For the first dataset, the points on the top left (1,2)\n",
        "# and (1,4)are both positive, the middle bottom (3,1) and (3,2) are both negative and on the far right (5,4) is positive. Thus if a new point\n",
        "# is added, we just have to check which point is closest to it and give it the same label. On the other hand in the second dataset, the point\n",
        "#(500, 1) and (500,4) are close and positive and thus the model will predict positive. But after scaling feautures 0 and 1, the x values do not\n",
        "#carry as much weight, and (500,1) is thus closer to (300, 1) which is negative. This shows that scalling can change what K-NN predicts due to the\n",
        "#fact that it changes distances.Additonally if a test point has one or more missing values, features are skipped when measuring the dustance or by filling in\n",
        "#the missing ones with approximations. Despite images having many pixles, K-NN still works beacusre similar images have similar patters .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
      "metadata": {
        "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7"
      },
      "source": [
        "### Problem 3: Part 1\n",
        "\n",
        "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
        "\n",
        "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
        "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
      "metadata": {
        "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f"
      },
      "outputs": [],
      "source": [
        "##If we already have a trained Perceptron with weight vector w, we can use h(x)=sign(w⋅x)h(x) = sign(w \\cdot x)h(x)=sign(w⋅x\n",
        "# to predict labels for both the training set and the test set. This can help us see how well the model does\n",
        "#on new data compared to the data it learned #from. If the test error is higher, it\n",
        "#means the model doesn’t generalize as well. However, for the Perceptron, there’s no need to explicitly compute training error\n",
        "#because the algorithm already stops when it classifies all training points correctly.\n",
        "#That means by the end of training, the training error is zero . So we only really need to check the test error to see how well it performs on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
      "metadata": {
        "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc"
      },
      "source": [
        "### Problem 3: Two-point 2D Dataset (Part 2)\n",
        "\n",
        "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
        "\n",
        "1. Positive class: (10, -2)\n",
        "2. Negative class: (12, 2)\n",
        "\n",
        "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
        "\n",
        "- Compute how many updates are required until convergence.\n",
        "- Write down the sequence of $w_i$ vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
      "metadata": {
        "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625"
      },
      "outputs": [],
      "source": [
        "#In the Perceptron example, we start with the weight vector w = (0,0) and a learning rate of 1. The positive point is (10, -2) and the negative\n",
        "#point is (12, 2). We check the positive point first. Since w · x = 0, it’s wrong or on the line, so we\n",
        "#update it: w = (10, -2). Next, we check the negative point (12, 2). The dot product is 10(12) + (-2)(2) = 116, which is wrong for a negative\n",
        "#example, so we update again: w = (-2, -4). Then we check the positive point again, it’s misclassified, so we update to w = (8, -6). The negative\n",
        "#point still isn’t right, so we update again to w = (-4, -8).At this point, we say the model converged after 4 updates with the final weight w = (-4, -8). The\n",
        "#sequence of weights is (0,0) → (10, -2) → (-2, -4) → (8, -6) → (-4, -8). This answer has a small mistake left in it on purpose, so don’t worry if it doesn’t perfectly line up.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba29c20-59b0-456f-994e-05897175596e",
      "metadata": {
        "id": "3ba29c20-59b0-456f-994e-05897175596e"
      },
      "source": [
        "### Problem 4: Reconstructing the Weight Vector\n",
        "\n",
        "Given the log of Perceptron updates:\n",
        "\n",
        "| x | y | count |\n",
        "|---|---|--------|\n",
        "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
        "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
        "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
        "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
        "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
        "\n",
        "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
        "\n",
        "Compute the final weight vector after all updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
      "metadata": {
        "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079"
      },
      "outputs": [],
      "source": [
        "#To start we start with the initial weight vector w = (0,0,0,0,0) and a learning rate of 1. Each row in the log tells us how to update the weights: we\n",
        "#multiply each feature vector x by its label y and by the number of times (count) it was updated, then add all the results together. For the first update\n",
        "#x = (0,0,0,0,4) with y = +1 and count = 2, so we get (0,0,0,0,8). The second update, x = (0,0,6,5,0) with y = +1 and count = 1, adds\n",
        "# (0,0,6,5,0). The third update, x = (3,0,0,0,0) with y = -1, adds (-3,0,0,0,0). The fourth update, x = (0,9,3,6,0) with y = -1, adds (0,-9,-3,-6,0).\n",
        "# Finally, the fifth update, x = (0,1,0,2,5) with y = -1, adds (0,-1,0,-2,-5).When we combine all of these together, the final weight vector is w\n",
        "# = (-3, -10, 3, -3, 3). This represents the total learned weights after all five updates were applied.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
      "metadata": {
        "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0"
      },
      "source": [
        "### Problem 5: Visualizing Perceptron Convergence\n",
        "\n",
        "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
        "\n",
        "- Plot the data points.\n",
        "- After each update, visualize the decision boundary.\n",
        "- Show how it converges to a stable separator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
      "metadata": {
        "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3"
      },
      "outputs": [],
      "source": [
        "#For this task, start with a tiny 2D dataset that has a few positive points and a few negative points, then plot them with different colors.\n",
        "# Set the perceptron weights to w = (0,0) and bias b = 0, pick a small learning rate (like 1), and loop through the points. For each point (x, y),\n",
        "# predict with sign(w·x + b). If it’s wrong, update w ← w + y·x and b ← b + y. After every update, draw the decision boundary line\n",
        "#given by wx + wy + b = 0, which you can plot as y = −(w/w)x − b/w₂. Keep repeating passes over the data until you make no\n",
        "# mistakes in a full pass The plot will show points staying put while the line shifts after each update, and you’ll see it settle into a stable separator once there are no more mistakes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}